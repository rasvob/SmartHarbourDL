{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and clean prediciton data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ground_truth = pd.read_csv('../annotations/ground_truth_boat_frames_coco.csv', index_col=0, sep=';')\n",
    "df_yolo_predictions = pd.read_csv('../annotations/yolov8x_no_train_labels.csv', index_col=0, sep=';')\n",
    "\n",
    "df_ground_truth['datetime'] = pd.to_datetime(df_ground_truth.datetime)\n",
    "df_ground_truth['date'] = df_ground_truth.datetime.dt.date\n",
    "df_yolo_predictions['datetime'] = pd.to_datetime(df_yolo_predictions['datetime'])\n",
    "df_yolo_predictions['date'] = pd.to_datetime(df_yolo_predictions.datetime.dt.date, format='%Y-%m-%d')\n",
    "print('shape of loaded data', df_ground_truth.shape, df_yolo_predictions.shape)\n",
    "print('Ground truth data:')\n",
    "print(df_ground_truth.groupby(['date', 'camera_id']).datetime.count())\n",
    "print('YOLO predictions data:')\n",
    "print(df_yolo_predictions.groupby(['date', 'camera_id']).datetime.count())\n",
    "\n",
    "df_yolo_predictions = df_yolo_predictions[\\\n",
    "    ((df_yolo_predictions.date == '2023-06-09') & (df_yolo_predictions.camera_id.isin([1,2]))) |\\\n",
    "    ((df_yolo_predictions.date == '2023-06-10') & (df_yolo_predictions.camera_id == 2)) |\\\n",
    "    ((df_yolo_predictions.date == '2023-07-07') & (df_yolo_predictions.camera_id == 2)) |\\\n",
    "    ((df_yolo_predictions.date == '2023-07-08') & (df_yolo_predictions.camera_id == 1)) \\\n",
    "].copy()\n",
    "\n",
    "# crop bounding boxes from right side of camera 2 field of view\n",
    "df_yolo_predictions.drop(index=df_yolo_predictions[(df_yolo_predictions.camera_id == 2) & (df_yolo_predictions.x > 1800)].index, inplace=True)\n",
    "df_ground_truth.drop(index=df_ground_truth[(df_ground_truth.camera_id == 2) & (df_ground_truth.x > 1800)].index, inplace=True)\n",
    "\n",
    "df_ground_truth.set_index('filename', inplace=True)\n",
    "df_yolo_predictions.set_index('filename', inplace=True)\n",
    "\n",
    "df_ground_truth.shape, df_yolo_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out predicitons in the bank on camera 1; use a line to define the bank in the camera 1 field of view\n",
    "point1_cam01_bank = (489, 591)\n",
    "point2_cam01_bank = (1499, 875)\n",
    "slope_cam01_bank = (point2_cam01_bank[1] - point1_cam01_bank[1]) / (point2_cam01_bank[0] - point1_cam01_bank[0])\n",
    "intercept_cam01_bank = point1_cam01_bank[1] - slope_cam01_bank * point1_cam01_bank[0]\n",
    "\n",
    "def under_the_bank_apply(row):\n",
    "    if row.camera_id == 1 and row.x >= point1_cam01_bank[0] and row.x <= point2_cam01_bank[0]:\n",
    "        if row.y > (slope_cam01_bank * row.x + intercept_cam01_bank):\n",
    "            return True        \n",
    "    return False\n",
    "        \n",
    "df_yolo_predictions['under_the_bank'] = df_yolo_predictions.apply(under_the_bank_apply, axis=1)\n",
    "print('How many prediction were under the bank?')\n",
    "print(df_yolo_predictions['under_the_bank'].value_counts())\n",
    "df_yolo_predictions = df_yolo_predictions[df_yolo_predictions.under_the_bank == False].copy()\n",
    "print('YOLO predictions filtered data:')\n",
    "print(df_yolo_predictions.groupby(['date', 'camera_id']).datetime.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation precision recall of detected frame_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(ground_truth:tuple, prediction:tuple):\n",
    "    \"\"\"\n",
    "        Calculate intersection over union for two bounding boxes.\n",
    "        Args:\n",
    "            ground_truth: tuple of (x, y, w, h)\n",
    "            prediction: tuple of (x, y, w, h)\n",
    "    \"\"\"\n",
    "    gt_xtl = ground_truth[0]-ground_truth[2]/2\n",
    "    gt_ytl = ground_truth[1]-ground_truth[3]/2\n",
    "    gt_xbr = ground_truth[0]+ground_truth[2]/2\n",
    "    gt_ybr = ground_truth[1]+ground_truth[3]/2\n",
    "    pr_xtl = prediction[0]-prediction[2]/2\n",
    "    pr_ytl = prediction[1]-prediction[3]/2\n",
    "    pr_xbr = prediction[0]+prediction[2]/2\n",
    "    pr_ybr = prediction[1]+prediction[3]/2\n",
    "    intersection_xtl = max(gt_xtl, pr_xtl)\n",
    "    intersection_ytl = max(gt_ytl, pr_ytl)\n",
    "    intersection_xbr = min(gt_xbr, pr_xbr)\n",
    "    intersection_ybr = min(gt_ybr, pr_ybr)\n",
    "    intersection_area = max(0, intersection_xbr - intersection_xtl) * max(0, intersection_ybr - intersection_ytl)\n",
    "    union_area = ground_truth[2] * ground_truth[3] + prediction[2] * prediction[3] - intersection_area\n",
    "    return intersection_area / union_area\n",
    "\n",
    "def evaluate_model(df_ground_truth, df_predictions):\n",
    "    df_predictions_frame_indexed = df_predictions.reset_index().set_index(['filename', 'frame_id'])\n",
    "    # group quality results by name, aggregate over frame_id and calculate true positive, false positive, false negative when comparing corresponding names and frame from both dataset\n",
    "    evaluation_dict = dict()\n",
    "    for id in tqdm.tqdm(set(df_ground_truth.index) | set(df_predictions.index)):\n",
    "        evaluation_dict[id] = dict()\n",
    "\n",
    "        if id in df_ground_truth.index:\n",
    "            if df_ground_truth.loc[id,'frame_id'].size == 1:\n",
    "                ground_truth_frame_ids = set([df_ground_truth.loc[id,'frame_id']])\n",
    "            else:\n",
    "                ground_truth_frame_ids = set(df_ground_truth.loc[id,'frame_id'])\n",
    "        else:\n",
    "            ground_truth_frame_ids = set()\n",
    "\n",
    "        if id in df_predictions.index:\n",
    "            if df_predictions.loc[id,'frame_id'].size == 1:\n",
    "                yolo_frame_ids = set([df_predictions.loc[id,'frame_id']])\n",
    "            else:\n",
    "                yolo_frame_ids = set(df_predictions.loc[id,'frame_id'])\n",
    "        else:\n",
    "            yolo_frame_ids = set()\n",
    "        \n",
    "        corresponding_frames = ground_truth_frame_ids & yolo_frame_ids\n",
    "        evaluation_dict[id]['true_positive'] = len(corresponding_frames)\n",
    "        evaluation_dict[id]['false_positive'] = len(yolo_frame_ids - ground_truth_frame_ids)\n",
    "        evaluation_dict[id]['false_negative'] = len(ground_truth_frame_ids - yolo_frame_ids)\n",
    "        if len(corresponding_frames) > 0:\n",
    "            frames_iou = {}\n",
    "            for frame_id in corresponding_frames:\n",
    "                # calulate iou for each frame\n",
    "                ground_truth_frame = df_ground_truth.loc[id].loc[df_ground_truth.loc[id].frame_id == frame_id].iloc[0]\n",
    "                # prediction_frame = df_predictions.loc[id].loc[df_predictions.loc[id].frame_id == frame_id].sort_values('w', ascending=False).iloc[0] ## this line was computation heavy, therefore indexed version is used\n",
    "                prediciton_frames = df_predictions_frame_indexed.loc[id].loc[frame_id]\n",
    "                if len(prediciton_frames.shape) == 1:\n",
    "                    prediction_frame = prediciton_frames\n",
    "                else:\n",
    "                    prediction_frame = prediciton_frames.sort_values(['confidence', 'w'], ascending=False).iloc[0]\n",
    "                frames_iou[frame_id] = calculate_iou(ground_truth_frame[['x', 'y', 'w', 'h']].values, prediction_frame[['x', 'y', 'w', 'h']].values)\n",
    "            evaluation_dict[id]['iou'] = sum(frames_iou.values()) / len(frames_iou.values())\n",
    "            evaluation_dict[id]['frames_iou'] = frames_iou\n",
    "        else:\n",
    "            evaluation_dict[id]['iou'] = 0\n",
    "            evaluation_dict[id]['frames_iou'] = []\n",
    "\n",
    "    df_evaluation = pd.DataFrame().from_dict(evaluation_dict, orient='index')\n",
    "    df_evaluation['f1'] = 2 * df_evaluation['true_positive'] / (2 * df_evaluation['true_positive'] + df_evaluation['false_positive'] + df_evaluation['false_negative'])\n",
    "    df_evaluation['recall'] = df_evaluation['true_positive'] / (df_evaluation['true_positive'] + df_evaluation['false_negative'])\n",
    "    df_evaluation['precision'] = df_evaluation['true_positive'] / (df_evaluation['true_positive'] + df_evaluation['false_positive'])\n",
    "    \n",
    "    total_eval = df_evaluation[['true_positive','false_positive','false_negative']].sum(axis=0)\n",
    "    total_evaluation_dict = {\n",
    "        'f1': 2*total_eval['true_positive'] / (2*total_eval['true_positive'] + total_eval['false_positive'] + total_eval['false_negative']),\n",
    "        'recall': total_eval['true_positive'] / (total_eval['true_positive'] + total_eval['false_negative']),\n",
    "        'precision': total_eval['true_positive'] / (total_eval['true_positive'] + total_eval['false_positive']),\n",
    "        'iou': (df_evaluation['true_positive']*df_evaluation['iou']).sum() / total_eval['true_positive']\n",
    "    }\n",
    "    return df_evaluation, total_evaluation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation, total_evaluation_dict = evaluate_model(df_ground_truth, df_yolo_predictions)\n",
    "total_evaluation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation.to_csv('data_evaluation.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence values evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_threshold_evaluation_dict = dict()\n",
    "for confidence_threshold in [0.25, 0.5, 0.75, 0.9]:\n",
    "    print('Confidence threshold', confidence_threshold)\n",
    "    df_evaluation, total_evaluation_dict = evaluate_model(df_ground_truth, df_yolo_predictions[df_yolo_predictions.confidence >= confidence_threshold])    \n",
    "    print(total_evaluation_dict)\n",
    "    # df_evaluation.to_csv(f'data_evaluation_conf{confidence_threshold}.csv', sep=';')\n",
    "    confidence_threshold_evaluation_dict[confidence_threshold] = total_evaluation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(confidence_threshold_evaluation_dict.keys(), [total_evaluation_dict['recall'] for total_evaluation_dict in confidence_threshold_evaluation_dict.values()], 'x-')\n",
    "plt.xlabel('Confidence threshold')\n",
    "plt.ylabel('Recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(confidence_threshold_evaluation_dict.keys(), [total_evaluation_dict['f1'] for total_evaluation_dict in confidence_threshold_evaluation_dict.values()], 'x-')\n",
    "plt.xlabel('Confidence threshold')\n",
    "plt.ylabel('F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(confidence_threshold_evaluation_dict.keys(), [total_evaluation_dict['iou'] for total_evaluation_dict in confidence_threshold_evaluation_dict.values()], 'x-')\n",
    "plt.xlabel('Confidence threshold')\n",
    "plt.ylabel('IOU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mAP evaluation\n",
    "- Use https://github.com/bes-dev/mean_average_precision.git for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform coords to xmin, ymin, xmax, ymax\n",
    "df_ground_truth['xmin'] = (df_ground_truth['x'] - df_ground_truth['w']/2).astype(int)\n",
    "df_ground_truth['ymin'] = (df_ground_truth['y'] - df_ground_truth['h']/2).astype(int)\n",
    "df_ground_truth['xmax'] = (df_ground_truth['x'] + df_ground_truth['w']/2).astype(int)\n",
    "df_ground_truth['ymax'] = (df_ground_truth['y'] + df_ground_truth['h']/2).astype(int)\n",
    "df_yolo_predictions['xmin'] = (df_yolo_predictions['x'] - df_yolo_predictions['w']/2).astype(int)\n",
    "df_yolo_predictions['ymin'] = (df_yolo_predictions['y'] - df_yolo_predictions['h']/2).astype(int)\n",
    "df_yolo_predictions['xmax'] = (df_yolo_predictions['x'] + df_yolo_predictions['w']/2).astype(int)\n",
    "df_yolo_predictions['ymax'] = (df_yolo_predictions['y'] + df_yolo_predictions['h']/2).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firt evaluation\n",
    "- Evaluation only for videos (10 minutes) with known ground truths and predictions\n",
    "- Missing frames from ground truth or prediction is threated as wrong prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_fn = MetricBuilder.build_evaluation_metric(\"map_2d\", async_mode=True, num_classes=1)\n",
    "\n",
    "# for id in tqdm.tqdm(set(df_ground_truth.index) | set(df_yolo_predictions.index)):\n",
    "for id in tqdm.tqdm(set(df_ground_truth.index) & set(df_yolo_predictions.index)):\n",
    "    # print(id)\n",
    "    # TODO there can be an error bcs id not in index\n",
    "    df_ground_truth_filtered = df_ground_truth.loc[id].set_index('frame_id')\n",
    "    df_yolo_predictions_filtered = df_yolo_predictions.loc[id].set_index('frame_id')\n",
    "\n",
    "    # if id in df_ground_truth.index:\n",
    "    #     if df_ground_truth.loc[id,'frame_id'].size == 1:\n",
    "    #         ground_truth_frame_ids = set([df_ground_truth.loc[id,'frame_id']])\n",
    "    #     else:\n",
    "    #         ground_truth_frame_ids = set(df_ground_truth.loc[id,'frame_id'])\n",
    "    # else:\n",
    "    #     ground_truth_frame_ids = set()\n",
    "\n",
    "    # if id in df_yolo_predictions.index:\n",
    "    #     if df_yolo_predictions.loc[id,'frame_id'].size == 1:\n",
    "    #         yolo_frame_ids = set([df_yolo_predictions.loc[id,'frame_id']])\n",
    "    #     else:\n",
    "    #         yolo_frame_ids = set(df_yolo_predictions.loc[id,'frame_id'])\n",
    "    # else:\n",
    "    #     yolo_frame_ids = set()\n",
    "\n",
    "    for frame_id in list(df_ground_truth_filtered.index.union(df_yolo_predictions_filtered.index)):\n",
    "        if frame_id in df_ground_truth_filtered.index:            \n",
    "            gt = df_ground_truth_filtered.loc[frame_id][['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "        else:\n",
    "            gt = np.array([0, 0, 0, 0])\n",
    "        if frame_id in df_yolo_predictions_filtered.index:\n",
    "            preds = df_yolo_predictions_filtered.loc[frame_id][['xmin', 'ymin', 'xmax', 'ymax', 'confidence']].values\n",
    "        else:\n",
    "            preds = np.array([0, 0, 0, 0, 0])\n",
    "\n",
    "        if len(gt.shape) == 1:\n",
    "            gt = np.array([gt])\n",
    "        if len(preds.shape) == 1:\n",
    "            preds = np.array([preds])\n",
    "        # gt [xmin, ymin, xmax, ymax, class_id, difficult, crowd]\n",
    "        # preds [xmin, ymin, xmax, ymax, class_id, confidence]\n",
    "        gt = np.concatenate((gt, np.zeros((gt.shape[0], 3))), axis=1)\n",
    "        preds = np.concatenate((preds, np.zeros((preds.shape[0], 1))), axis=1)\n",
    "        preds[:, -1] = preds[:, -2]\n",
    "        preds[:, -2] = 0\n",
    "\n",
    "        metric_fn.add(preds, gt)\n",
    "\n",
    "# compute PASCAL VOC metric\n",
    "print(f\"VOC PASCAL mAP: {metric_fn.value(iou_thresholds=0.5, recall_thresholds=np.arange(0., 1.1, 0.1))['mAP']}\")\n",
    "# compute PASCAL VOC metric at the all points\n",
    "print(f\"VOC PASCAL mAP in all points: {metric_fn.value(iou_thresholds=0.5)['mAP']}\")\n",
    "# compute metric COCO metric\n",
    "print(f\"COCO mAP: {metric_fn.value(iou_thresholds=np.arange(0.5, 1.0, 0.05), recall_thresholds=np.arange(0., 1.01, 0.01), mpolicy='soft')['mAP']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate only for ground truth frames - similar to finetuning procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_fn = MetricBuilder.build_evaluation_metric(\"map_2d\", async_mode=True, num_classes=1)\n",
    "\n",
    "for id in tqdm.tqdm(set(df_ground_truth.index) & set(df_yolo_predictions.index)):\n",
    "    df_ground_truth_filtered = df_ground_truth.loc[id].set_index('frame_id')\n",
    "    df_yolo_predictions_filtered = df_yolo_predictions.loc[id].set_index('frame_id')\n",
    "\n",
    "    for frame_id in list(df_ground_truth_filtered.index):\n",
    "        if frame_id in df_ground_truth_filtered.index:            \n",
    "            gt = df_ground_truth_filtered.loc[frame_id][['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "        else:\n",
    "            gt = np.array([0, 0, 0, 0])\n",
    "        if frame_id in df_yolo_predictions_filtered.index:\n",
    "            preds = df_yolo_predictions_filtered.loc[frame_id][['xmin', 'ymin', 'xmax', 'ymax', 'confidence']].values\n",
    "        else:\n",
    "            preds = np.array([0, 0, 0, 0, 0])\n",
    "\n",
    "        if len(gt.shape) == 1:\n",
    "            gt = np.array([gt])\n",
    "        if len(preds.shape) == 1:\n",
    "            preds = np.array([preds])\n",
    "        # gt [xmin, ymin, xmax, ymax, class_id, difficult, crowd]\n",
    "        # preds [xmin, ymin, xmax, ymax, class_id, confidence]\n",
    "        gt = np.concatenate((gt, np.zeros((gt.shape[0], 3))), axis=1)\n",
    "        preds = np.concatenate((preds, np.zeros((preds.shape[0], 1))), axis=1)\n",
    "        preds[:, -1] = preds[:, -2]\n",
    "        preds[:, -2] = 0\n",
    "\n",
    "        metric_fn.add(preds, gt)\n",
    "\n",
    "# compute PASCAL VOC metric\n",
    "print(f\"VOC PASCAL mAP: {metric_fn.value(iou_thresholds=0.5, recall_thresholds=np.arange(0., 1.1, 0.1))['mAP']}\")\n",
    "# compute PASCAL VOC metric at the all points\n",
    "print(f\"VOC PASCAL mAP in all points: {metric_fn.value(iou_thresholds=0.5)['mAP']}\")\n",
    "# compute metric COCO metric\n",
    "print(f\"COCO mAP: {metric_fn.value(iou_thresholds=np.arange(0.5, 1.0, 0.05), recall_thresholds=np.arange(0., 1.01, 0.01), mpolicy='soft')['mAP']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate only for ground truth frames - use only validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ground_truth['date'] = pd.to_datetime(df_ground_truth.datetime.dt.date, format='%Y-%m-%d')\n",
    "df_ground_truth.shape, df_ground_truth[(df_ground_truth.date == '2023-07-07') | (df_ground_truth.date == '2023-07-08')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_fn = MetricBuilder.build_evaluation_metric(\"map_2d\", async_mode=True, num_classes=1)\n",
    "\n",
    "for id in tqdm.tqdm(set(df_ground_truth[(df_ground_truth.date == '2023-07-07') | (df_ground_truth.date == '2023-07-08')].index) & set(df_yolo_predictions.index)):\n",
    "    df_ground_truth_filtered = df_ground_truth.loc[id].set_index('frame_id')\n",
    "    df_yolo_predictions_filtered = df_yolo_predictions.loc[id].set_index('frame_id')\n",
    "\n",
    "    for frame_id in list(df_ground_truth_filtered.index):\n",
    "        if frame_id in df_ground_truth_filtered.index:            \n",
    "            gt = df_ground_truth_filtered.loc[frame_id][['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "        else:\n",
    "            gt = np.array([0, 0, 0, 0])\n",
    "        if frame_id in df_yolo_predictions_filtered.index:\n",
    "            preds = df_yolo_predictions_filtered.loc[frame_id][['xmin', 'ymin', 'xmax', 'ymax', 'confidence']].values\n",
    "        else:\n",
    "            preds = np.array([0, 0, 0, 0, 0])\n",
    "\n",
    "        if len(gt.shape) == 1:\n",
    "            gt = np.array([gt])\n",
    "        if len(preds.shape) == 1:\n",
    "            preds = np.array([preds])\n",
    "        # gt [xmin, ymin, xmax, ymax, class_id, difficult, crowd]\n",
    "        # preds [xmin, ymin, xmax, ymax, class_id, confidence]\n",
    "        gt = np.concatenate((gt, np.zeros((gt.shape[0], 3))), axis=1)\n",
    "        preds = np.concatenate((preds, np.zeros((preds.shape[0], 1))), axis=1)\n",
    "        preds[:, -1] = preds[:, -2]\n",
    "        preds[:, -2] = 0\n",
    "\n",
    "        metric_fn.add(preds, gt)\n",
    "\n",
    "# compute PASCAL VOC metric\n",
    "print(f\"VOC PASCAL mAP: {metric_fn.value(iou_thresholds=0.5, recall_thresholds=np.arange(0., 1.1, 0.1))['mAP']}\")\n",
    "# compute PASCAL VOC metric at the all points\n",
    "print(f\"VOC PASCAL mAP in all points: {metric_fn.value(iou_thresholds=0.5)['mAP']}\")\n",
    "# compute metric COCO metric\n",
    "print(f\"COCO mAP: {metric_fn.value(iou_thresholds=np.arange(0.5, 1.0, 0.05), recall_thresholds=np.arange(0., 1.01, 0.01), mpolicy='soft')['mAP']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cfg_raw_cam_02_fhd_h265_20230609T173000 # there is a car identified as a boat outside of the field of view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr(x):\n",
    "    return np.percentile(x, 75) - np.percentile(x, 25)\n",
    "# filter out static bounding box predictions; use iqr < 100 as a threshold and later shift of x coords are less than 10\n",
    "df_tmp = df_yolo_predictions[df_yolo_predictions.camera_id == 2].reset_index()\n",
    "df_tmp = df_tmp.groupby('filename').datetime.count()\n",
    "df_tmp = df_tmp[df_tmp > 100]\n",
    "df_tmp = df_yolo_predictions.loc[df_tmp.index].reset_index().groupby('filename').agg({'x': iqr, 'y': iqr})\n",
    "df_static_indexes = df_tmp[(df_tmp.x < 100) & (df_tmp.y < 100)].index\n",
    "\n",
    "# for each static filename, calculate the shift of x coords between frames and filter out those with shift > 10\n",
    "for filename in list(df_static_indexes)[5:]:\n",
    "    print(filename)\n",
    "    df_tmp = df_yolo_predictions.loc[filename].groupby('frame_id').x.max().reset_index()\n",
    "    # df_tmp = \n",
    "    df_tmp['x_shift'] = df_tmp['x'].diff().abs()\n",
    "    print(df_tmp)\n",
    "    # df_tmp = df_tmp[df_tmp['x_shift'] > 10]\n",
    "    # print(df_tmp)\n",
    "    # df_static_indexes = df_static_indexes.drop(filename)\n",
    "    # df_static_indexes = df_static_indexes.append(df_tmp.index)\n",
    "\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "# .agg({'x': lambda x: np.percentile(x, 75) - np.percentile(x, 25)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values for camera 2 bank identification.. this should be the water.. camera have a good horizontal angle, therefore check that bottom coord of bounding box is in upper area of the image y < less than straight line\n",
    "x = 2 (0.001042), y = 378 (0.350000)\n",
    "x = 1918 (0.998958), y = 445 (0.412037)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.set_index('frame_id').to_csv('debug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.loc['cfg_raw_cam_02_fhd_h265_20230707T202002.mkv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_yolo_predictions.loc['cfg_raw_cam_02_fhd_h265_20230707T202002.mkv'].groupby('frame_id').x.max() - df_yolo_predictions.loc['cfg_raw_cam_02_fhd_h265_20230707T202002.mkv'].groupby('frame_id').x.max().shift()).rolling(10).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1597  /4/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yolo_predictions.loc['cfg_raw_cam_02_fhd_h265_20230707T202002.mkv'][df_yolo_predictions.loc['cfg_raw_cam_02_fhd_h265_20230707T202002.mkv'].frame_id > 2250].sort_values('frame_id').head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yolo_predictions.loc['cfg_raw_cam_02_fhd_h265_20230707T202002.mkv'].groupby('frame_id').x.max().sort_index().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yolo_predictions.loc['cfg_raw_cam_02_fhd_h265_20230707T174000.mkv'].y.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yolo_predictions.loc['cfg_raw_cam_02_fhd_h265_20230707T174000.mkv'].x.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_evaluation.iloc[0].frames_iou.keys(), df_evaluation.iloc[0].frames_iou.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ground_truth.loc['cfg_raw_cam_01_fhd_h265_20230609T050002.mkv'].sort_values('frame_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ground_truth.loc['cfg_raw_cam_02_fhd_h265_20230707T124001.mkv'].sort_values('frame_id').frame_id.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_frame_id = 1087\n",
    "convert_frame_id / 4 / 60, convert_frame_id / 4 // 60, convert_frame_id / 4 % 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_yolo_predictions.loc['cfg_raw_cam_02_fhd_h265_20230707T124001.mkv']\n",
    "df_tmp[df_tmp.x < 1800].sort_values('frame_id').frame_id.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
